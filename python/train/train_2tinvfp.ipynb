{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリのインポート\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, models\n",
    "from keras.callbacks import TensorBoard, EarlyStopping\n",
    "from keras.layers import Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformerブロック\n",
    "class TransformerBlock(layers.Layer):\n",
    "    # 初期化\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation=\"relu\"),\n",
    "            layers.Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    # 呼び出し\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)                      # マルチヘッドアテンション層\n",
    "        attn_output = self.dropout1(attn_output, training=training) # ドロップアウト\n",
    "        out1 = self.layernorm1(inputs + attn_output)                # レイヤー正規化\n",
    "        ffn_output = self.ffn(out1)                                 # フィードフォワードネットワーク層\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)   # ドロップアウト\n",
    "        return self.layernorm2(out1 + ffn_output)                   # レイヤー正規化\n",
    "\n",
    "# Transformerモデル\n",
    "class TransformerModel(models.Model):\n",
    "    # 初期化\n",
    "    def __init__(self, config):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.config = config\n",
    "        self.normalizer = Normalization(axis=-1)\n",
    "        self.input_projection = layers.Dense(config['embed_dim'])\n",
    "        self.transformer_blocks = [\n",
    "            TransformerBlock(config['embed_dim'], config['num_heads'], config['ff_dim'], config['dropout_rate'])\n",
    "            for _ in range(config['num_transformer_blocks'])\n",
    "        ]\n",
    "        self.global_average_pooling = layers.GlobalAveragePooling1D()\n",
    "        self.dropout = layers.Dropout(config['dropout_rate'])\n",
    "        self.dense_layers = [layers.Dense(units, activation=\"relu\") for units in config['dense_units']]\n",
    "        self.output_layer = layers.Dense(2)\n",
    "\n",
    "    # 呼び出し\n",
    "    def call(self, inputs):\n",
    "        x = self.normalizer(inputs)                         # 訓練データの正規化\n",
    "        x = self.input_projection(x)                        # 入力の投影\n",
    "        x = tf.expand_dims(x, axis=1)                       # 2次元→3次元の拡張\n",
    "        for transformer_block in self.transformer_blocks:   # Transformerブロック\n",
    "            x = transformer_block(x)\n",
    "        x = self.global_average_pooling(x)                  # 1次元に\n",
    "        x = self.dropout(x)                                 # ドロップアウト\n",
    "        for dense_layer in self.dense_layers:               # 全結合層\n",
    "            x = dense_layer(x)\n",
    "            x = self.dropout(x)\n",
    "        return self.output_layer(x)                         # 出力層(2次元)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練\n",
    "class TransformerTrainer:\n",
    "    # 初期化\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "\n",
    "    # データの前処理\n",
    "    def preprocess_data(self, dataset):\n",
    "        data = dataset.iloc[:, list(range(4, 12)) + [29] + list(range(19, 27))]\n",
    "        label = dataset[['x_2', 'y_2']]\n",
    "        return data.values, label.values\n",
    "\n",
    "    # モデルのビルドとコンパイル\n",
    "    def build_model(self, input_shape):\n",
    "        self.model = TransformerModel(self.config)\n",
    "        self.model.build(input_shape)\n",
    "        self.model.summary()\n",
    "        \n",
    "        # 学習率のスケジューリング\n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "            self.config['initial_learning_rate'],\n",
    "            decay_steps=self.config['decay_steps'],\n",
    "            decay_rate=self.config['decay_rate'],\n",
    "            staircase=True)\n",
    "\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        self.model.compile(optimizer=optimizer, loss=\"mean_squared_error\", metrics=[\"mean_absolute_error\"])\n",
    "\n",
    "    # 訓練\n",
    "    def train(self, train_dataset, validation_split=0.2):\n",
    "        # 訓練データの前処理\n",
    "        data, label = self.preprocess_data(train_dataset)\n",
    "        \n",
    "        # サマリーの表示\n",
    "        if self.model is None:\n",
    "            self.build_model(data.shape)\n",
    "        else:\n",
    "            self.model.summary()\n",
    "        \n",
    "        # 訓練データの正規化\n",
    "        self.model.normalizer.adapt(data)\n",
    "        \n",
    "        # コールバック関数の定義\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=self.config['patience']),\n",
    "            TensorBoard(log_dir=self.config['log_dir'])\n",
    "        ]\n",
    "\n",
    "        # データの保存\n",
    "        history = self.model.fit(\n",
    "            data, label,\n",
    "            validation_split=validation_split,\n",
    "            batch_size=self.config['batch_size'],\n",
    "            epochs=self.config['epochs'],\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        self.model.save(self.config['model_path'])\n",
    "        return history\n",
    "\n",
    "    # 予測\n",
    "    def predict(self, data):\n",
    "        preprocessed_data, _ = self.preprocess_data(data)\n",
    "        return self.model.predict(preprocessed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizatio  multiple                 35        \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  1152      \n",
      "                                                                 \n",
      " transformer_block (Transfor  multiple                 83200     \n",
      " merBlock)                                                       \n",
      "                                                                 \n",
      " global_average_pooling1d (G  multiple                 0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         multiple                  0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             multiple                  4160      \n",
      "                                                                 \n",
      " dense_4 (Dense)             multiple                  2080      \n",
      "                                                                 \n",
      " dense_5 (Dense)             multiple                  66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 90,693\n",
      "Trainable params: 90,658\n",
      "Non-trainable params: 35\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "698/698 [==============================] - 11s 11ms/step - loss: 0.8630 - mean_absolute_error: 0.6798 - val_loss: 0.4316 - val_mean_absolute_error: 0.4815\n",
      "Epoch 2/10\n",
      "698/698 [==============================] - 8s 11ms/step - loss: 0.4057 - mean_absolute_error: 0.4737 - val_loss: 0.1936 - val_mean_absolute_error: 0.3145\n",
      "Epoch 3/10\n",
      "698/698 [==============================] - 7s 10ms/step - loss: 0.2732 - mean_absolute_error: 0.3835 - val_loss: 0.1042 - val_mean_absolute_error: 0.2372\n",
      "Epoch 4/10\n",
      "698/698 [==============================] - 7s 11ms/step - loss: 0.2021 - mean_absolute_error: 0.3247 - val_loss: 0.0479 - val_mean_absolute_error: 0.1540\n",
      "Epoch 5/10\n",
      "698/698 [==============================] - 7s 10ms/step - loss: 0.1602 - mean_absolute_error: 0.2863 - val_loss: 0.0345 - val_mean_absolute_error: 0.1302\n",
      "Epoch 6/10\n",
      "698/698 [==============================] - 7s 10ms/step - loss: 0.1343 - mean_absolute_error: 0.2591 - val_loss: 0.0255 - val_mean_absolute_error: 0.1117\n",
      "Epoch 7/10\n",
      "698/698 [==============================] - 7s 11ms/step - loss: 0.1158 - mean_absolute_error: 0.2377 - val_loss: 0.0178 - val_mean_absolute_error: 0.0934\n",
      "Epoch 8/10\n",
      "698/698 [==============================] - 7s 11ms/step - loss: 0.1026 - mean_absolute_error: 0.2218 - val_loss: 0.0119 - val_mean_absolute_error: 0.0767\n",
      "Epoch 9/10\n",
      "698/698 [==============================] - 8s 11ms/step - loss: 0.0922 - mean_absolute_error: 0.2103 - val_loss: 0.0093 - val_mean_absolute_error: 0.0692\n",
      "Epoch 10/10\n",
      "698/698 [==============================] - 8s 11ms/step - loss: 0.0844 - mean_absolute_error: 0.2012 - val_loss: 0.0097 - val_mean_absolute_error: 0.0701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as multi_head_attention_layer_call_fn, multi_head_attention_layer_call_and_return_conditional_losses, layer_normalization_layer_call_fn, layer_normalization_layer_call_and_return_conditional_losses, layer_normalization_1_layer_call_fn while saving (showing 5 of 22). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:/Users/sukegawa/Desktop/study/model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:/Users/sukegawa/Desktop/study/model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - 1s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# 設定\n",
    "config = {\n",
    "    'embed_dim': 64,\n",
    "    'num_heads': 4,\n",
    "    'ff_dim': 128,\n",
    "    'num_transformer_blocks': 1,\n",
    "    'dropout_rate': 0.1,\n",
    "    'dense_units': [64, 32],\n",
    "    'batch_size': 1024,\n",
    "    'epochs': 10,\n",
    "    'patience': 20,\n",
    "    'initial_learning_rate': 1e-3,\n",
    "    'decay_steps': 10000,\n",
    "    'decay_rate': 0.9,\n",
    "    'log_dir': R'C:/Users/sukegawa/Desktop/study/logs',\n",
    "    'model_path': R'C:/Users/sukegawa/Desktop/study/model'\n",
    "}\n",
    "\n",
    "# 訓練データの訓練\n",
    "estimator = TransformerTrainer(config)\n",
    "train_dataset = pd.read_csv(R'C:/Users/sukegawa/Desktop/study/datasets/tinvfp/tinvfp_train0.csv')\n",
    "history = estimator.train(train_dataset)\n",
    "\n",
    "# テストデータの評価\n",
    "test_data = pd.read_csv(R'C:/Users/sukegawa/Desktop/study/datasets/tinvfp/tinvfp_test0.csv')\n",
    "predictions = estimator.predict(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
