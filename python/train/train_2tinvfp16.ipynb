{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリのインポート\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, models\n",
    "from keras.callbacks import TensorBoard, EarlyStopping\n",
    "from keras.layers import Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformerブロック\n",
    "class TransformerBlock(layers.Layer):\n",
    "    # 初期化\n",
    "    def __init__(self, input_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=input_dim)\n",
    "        self.ffn = keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation=\"relu\"),\n",
    "            layers.Dense(input_dim),\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    # 呼び出し\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)                      # マルチヘッドアテンション層\n",
    "        attn_output = self.dropout1(attn_output, training=training) # ドロップアウト\n",
    "        out1 = self.layernorm1(inputs + attn_output)                # レイヤー正規化\n",
    "        ffn_output = self.ffn(out1)                                 # フィードフォワードネットワーク層\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)   # ドロップアウト\n",
    "        return self.layernorm2(out1 + ffn_output)                   # レイヤー正規化\n",
    "\n",
    "# Transformerモデル\n",
    "class TransformerModel(models.Model):\n",
    "    # 初期化\n",
    "    def __init__(self, config):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.config = config\n",
    "        self.normalizer = Normalization(axis=-1)\n",
    "        self.transformer_blocks = [\n",
    "            TransformerBlock(config['input_dim'], config['num_heads'], config['ff_dim'], config['dropout_rate'])\n",
    "            for _ in range(config['num_transformer_blocks'])\n",
    "        ]\n",
    "        self.global_average_pooling = layers.GlobalAveragePooling1D()\n",
    "        self.dropout = layers.Dropout(config['dropout_rate'])\n",
    "        self.dense_layers = [layers.Dense(units, activation=\"relu\") for units in config['dense_units']]\n",
    "        self.output_layer = layers.Dense(2)\n",
    "\n",
    "    # 呼び出し\n",
    "    def call(self, inputs):\n",
    "        x = self.normalizer(inputs)                         # 訓練データの正規化\n",
    "        for transformer_block in self.transformer_blocks:   # Transformerブロック\n",
    "            x = transformer_block(x)\n",
    "        x = self.global_average_pooling(x)                  # 1次元に\n",
    "        x = self.dropout(x)                                 # ドロップアウト\n",
    "        for dense_layer in self.dense_layers:               # 全結合層\n",
    "            x = dense_layer(x)\n",
    "            x = self.dropout(x)\n",
    "        return self.output_layer(x)                         # 出力層(2次元)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練\n",
    "class TransformerTrainer:\n",
    "    # 初期化\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "\n",
    "    # データの前処理\n",
    "    def preprocess_data(self, dataset):\n",
    "        data = dataset.iloc[:, list(range(4, 20)) + [22] + list(range(27, 43))]\n",
    "        label = dataset[['x_2', 'y_2']]\n",
    "        return data.values.reshape(-1, 1, 33), label.values\n",
    "\n",
    "    # モデルのビルドとコンパイル\n",
    "    def build_model(self, input_shape):\n",
    "        self.model = TransformerModel(self.config)\n",
    "        self.model.build(input_shape)\n",
    "        self.model.summary()\n",
    "        \n",
    "        # 学習率のスケジューリング\n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "            self.config['initial_learning_rate'],\n",
    "            decay_steps=self.config['decay_steps'],\n",
    "            decay_rate=self.config['decay_rate'],\n",
    "            staircase=True)\n",
    "\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        self.model.compile(optimizer=optimizer, loss=\"mean_squared_error\", metrics=[\"accuracy\"])\n",
    "\n",
    "    # 訓練\n",
    "    def train(self, train_dataset, validation_split=0.2):\n",
    "        # 訓練データの前処理\n",
    "        data, label = self.preprocess_data(train_dataset)\n",
    "        \n",
    "        # サマリーの表示\n",
    "        if self.model is None:\n",
    "            self.build_model(data.shape)\n",
    "        else:\n",
    "            self.model.summary()\n",
    "        \n",
    "        # 訓練データの正規化\n",
    "        self.model.normalizer.adapt(data)\n",
    "        \n",
    "        # コールバック関数の定義\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=self.config['patience']),\n",
    "            TensorBoard(log_dir=self.config['log_dir'])\n",
    "        ]\n",
    "\n",
    "        # データの保存\n",
    "        history = self.model.fit(\n",
    "            data, label,\n",
    "            validation_split=validation_split,\n",
    "            batch_size=self.config['batch_size'],\n",
    "            epochs=self.config['epochs'],\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        self.model.save(self.config['model_path'])\n",
    "        return history\n",
    "\n",
    "    # 予測\n",
    "    def predict(self, data):\n",
    "        preprocessed_data, _ = self.preprocess_data(data)\n",
    "        return self.model.predict(preprocessed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizatio  multiple                 67        \n",
      " n)                                                              \n",
      "                                                                 \n",
      " transformer_block (Transfor  multiple                 26862     \n",
      " merBlock)                                                       \n",
      "                                                                 \n",
      " transformer_block_1 (Transf  multiple                 26862     \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " transformer_block_2 (Transf  multiple                 26862     \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " global_average_pooling1d (G  multiple                 0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         multiple                  0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             multiple                  2176      \n",
      "                                                                 \n",
      " dense_7 (Dense)             multiple                  2080      \n",
      "                                                                 \n",
      " dense_8 (Dense)             multiple                  66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 84,975\n",
      "Trainable params: 84,908\n",
      "Non-trainable params: 67\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "1424/1424 [==============================] - 32s 20ms/step - loss: 0.4064 - accuracy: 0.9360 - val_loss: 0.0442 - val_accuracy: 0.9564\n",
      "Epoch 2/1000\n",
      "1424/1424 [==============================] - 28s 20ms/step - loss: 0.1517 - accuracy: 0.9457 - val_loss: 0.0276 - val_accuracy: 0.9895\n",
      "Epoch 3/1000\n",
      "1424/1424 [==============================] - 29s 20ms/step - loss: 0.1076 - accuracy: 0.9441 - val_loss: 0.0195 - val_accuracy: 0.9718\n",
      "Epoch 4/1000\n",
      "1424/1424 [==============================] - 28s 20ms/step - loss: 0.0809 - accuracy: 0.9417 - val_loss: 0.0103 - val_accuracy: 0.9926\n",
      "Epoch 5/1000\n",
      "1424/1424 [==============================] - 29s 20ms/step - loss: 0.0677 - accuracy: 0.9422 - val_loss: 0.0127 - val_accuracy: 0.9995\n",
      "Epoch 6/1000\n",
      "1424/1424 [==============================] - 30s 21ms/step - loss: 0.0604 - accuracy: 0.9411 - val_loss: 0.0123 - val_accuracy: 0.9998\n",
      "Epoch 7/1000\n",
      "1424/1424 [==============================] - 30s 21ms/step - loss: 0.0556 - accuracy: 0.9400 - val_loss: 0.0073 - val_accuracy: 0.9999\n",
      "Epoch 8/1000\n",
      "1424/1424 [==============================] - 30s 21ms/step - loss: 0.0533 - accuracy: 0.9391 - val_loss: 0.0095 - val_accuracy: 0.9999\n",
      "Epoch 9/1000\n",
      "1424/1424 [==============================] - 30s 21ms/step - loss: 0.0522 - accuracy: 0.9389 - val_loss: 0.0073 - val_accuracy: 0.9984\n",
      "Epoch 10/1000\n",
      "1424/1424 [==============================] - 29s 21ms/step - loss: 0.0513 - accuracy: 0.9388 - val_loss: 0.0068 - val_accuracy: 0.9995\n",
      "Epoch 11/1000\n",
      "1424/1424 [==============================] - 30s 21ms/step - loss: 0.0506 - accuracy: 0.9389 - val_loss: 0.0071 - val_accuracy: 0.9999\n",
      "Epoch 12/1000\n",
      "1424/1424 [==============================] - 30s 21ms/step - loss: 0.0499 - accuracy: 0.9394 - val_loss: 0.0076 - val_accuracy: 0.9998\n",
      "Epoch 13/1000\n",
      "1424/1424 [==============================] - 30s 21ms/step - loss: 0.0494 - accuracy: 0.9399 - val_loss: 0.0064 - val_accuracy: 1.0000\n",
      "Epoch 14/1000\n",
      "1424/1424 [==============================] - 29s 20ms/step - loss: 0.0490 - accuracy: 0.9400 - val_loss: 0.0105 - val_accuracy: 0.9996\n",
      "Epoch 15/1000\n",
      "1424/1424 [==============================] - 29s 21ms/step - loss: 0.0483 - accuracy: 0.9401 - val_loss: 0.0095 - val_accuracy: 0.9993\n",
      "Epoch 16/1000\n",
      "1424/1424 [==============================] - 30s 21ms/step - loss: 0.0483 - accuracy: 0.9400 - val_loss: 0.0090 - val_accuracy: 0.9994\n",
      "Epoch 17/1000\n",
      "1424/1424 [==============================] - 29s 21ms/step - loss: 0.0481 - accuracy: 0.9399 - val_loss: 0.0076 - val_accuracy: 0.9997\n",
      "Epoch 18/1000\n",
      "1424/1424 [==============================] - 29s 20ms/step - loss: 0.0479 - accuracy: 0.9400 - val_loss: 0.0079 - val_accuracy: 0.9997\n",
      "Epoch 19/1000\n",
      "1424/1424 [==============================] - 29s 21ms/step - loss: 0.0480 - accuracy: 0.9402 - val_loss: 0.0107 - val_accuracy: 0.9994\n",
      "Epoch 20/1000\n",
      "1424/1424 [==============================] - 29s 20ms/step - loss: 0.0476 - accuracy: 0.9401 - val_loss: 0.0061 - val_accuracy: 0.9582\n",
      "Epoch 21/1000\n",
      "1424/1424 [==============================] - 29s 20ms/step - loss: 0.0474 - accuracy: 0.9400 - val_loss: 0.0085 - val_accuracy: 0.9999\n",
      "Epoch 22/1000\n",
      "1424/1424 [==============================] - 30s 21ms/step - loss: 0.0469 - accuracy: 0.9408 - val_loss: 0.0067 - val_accuracy: 0.9997\n",
      "Epoch 23/1000\n",
      "1424/1424 [==============================] - 30s 21ms/step - loss: 0.0463 - accuracy: 0.9418 - val_loss: 0.0063 - val_accuracy: 0.9998\n",
      "Epoch 24/1000\n",
      "1424/1424 [==============================] - 29s 20ms/step - loss: 0.0466 - accuracy: 0.9420 - val_loss: 0.0076 - val_accuracy: 0.9994\n",
      "Epoch 25/1000\n",
      "1424/1424 [==============================] - 29s 21ms/step - loss: 0.0459 - accuracy: 0.9421 - val_loss: 0.0090 - val_accuracy: 0.9996\n",
      "Epoch 26/1000\n",
      "1424/1424 [==============================] - 30s 21ms/step - loss: 0.0462 - accuracy: 0.9419 - val_loss: 0.0063 - val_accuracy: 1.0000\n",
      "Epoch 27/1000\n",
      "1424/1424 [==============================] - 29s 20ms/step - loss: 0.0458 - accuracy: 0.9424 - val_loss: 0.0065 - val_accuracy: 0.9631\n",
      "Epoch 28/1000\n",
      "1424/1424 [==============================] - 29s 20ms/step - loss: 0.0459 - accuracy: 0.9424 - val_loss: 0.0083 - val_accuracy: 0.9998\n",
      "Epoch 29/1000\n",
      "1424/1424 [==============================] - 30s 21ms/step - loss: 0.0458 - accuracy: 0.9420 - val_loss: 0.0060 - val_accuracy: 0.9990\n",
      "Epoch 30/1000\n",
      "1424/1424 [==============================] - 30s 21ms/step - loss: 0.0457 - accuracy: 0.9423 - val_loss: 0.0074 - val_accuracy: 0.9998\n",
      "Epoch 31/1000\n",
      "1424/1424 [==============================] - 29s 20ms/step - loss: 0.0457 - accuracy: 0.9420 - val_loss: 0.0079 - val_accuracy: 0.9998\n",
      "Epoch 32/1000\n",
      "1424/1424 [==============================] - 30s 21ms/step - loss: 0.0457 - accuracy: 0.9423 - val_loss: 0.0079 - val_accuracy: 0.9984\n",
      "Epoch 33/1000\n",
      "1424/1424 [==============================] - 30s 21ms/step - loss: 0.0457 - accuracy: 0.9421 - val_loss: 0.0089 - val_accuracy: 0.9992\n",
      "Epoch 34/1000\n",
      "1424/1424 [==============================] - 30s 21ms/step - loss: 0.0455 - accuracy: 0.9422 - val_loss: 0.0069 - val_accuracy: 0.9994\n",
      "Epoch 35/1000\n",
      "1424/1424 [==============================] - 29s 21ms/step - loss: 0.0457 - accuracy: 0.9422 - val_loss: 0.0073 - val_accuracy: 0.9997\n",
      "Epoch 36/1000\n",
      "1424/1424 [==============================] - 30s 21ms/step - loss: 0.0455 - accuracy: 0.9424 - val_loss: 0.0073 - val_accuracy: 0.9998\n",
      "Epoch 37/1000\n",
      "1424/1424 [==============================] - 30s 21ms/step - loss: 0.0454 - accuracy: 0.9422 - val_loss: 0.0056 - val_accuracy: 0.9999\n",
      "Epoch 38/1000\n",
      "1424/1424 [==============================] - 30s 21ms/step - loss: 0.0454 - accuracy: 0.9426 - val_loss: 0.0063 - val_accuracy: 1.0000\n",
      "Epoch 39/1000\n",
      "1424/1424 [==============================] - 29s 21ms/step - loss: 0.0454 - accuracy: 0.9424 - val_loss: 0.0083 - val_accuracy: 0.9999\n",
      "Epoch 40/1000\n",
      "1424/1424 [==============================] - 30s 21ms/step - loss: 0.0453 - accuracy: 0.9422 - val_loss: 0.0070 - val_accuracy: 0.9998\n",
      "Epoch 41/1000\n",
      "1424/1424 [==============================] - 30s 21ms/step - loss: 0.0452 - accuracy: 0.9423 - val_loss: 0.0069 - val_accuracy: 1.0000\n",
      "Epoch 42/1000\n",
      "1424/1424 [==============================] - 29s 21ms/step - loss: 0.0454 - accuracy: 0.9424 - val_loss: 0.0068 - val_accuracy: 0.9998\n",
      "Epoch 43/1000\n",
      "1424/1424 [==============================] - 30s 21ms/step - loss: 0.0454 - accuracy: 0.9423 - val_loss: 0.0105 - val_accuracy: 0.9999\n",
      "Epoch 44/1000\n",
      "1424/1424 [==============================] - 30s 21ms/step - loss: 0.0452 - accuracy: 0.9424 - val_loss: 0.0068 - val_accuracy: 0.9999\n",
      "Epoch 45/1000\n",
      "1424/1424 [==============================] - 28s 20ms/step - loss: 0.0453 - accuracy: 0.9426 - val_loss: 0.0080 - val_accuracy: 1.0000\n",
      "Epoch 46/1000\n",
      "1424/1424 [==============================] - 28s 20ms/step - loss: 0.0452 - accuracy: 0.9426 - val_loss: 0.0054 - val_accuracy: 1.0000\n",
      "Epoch 47/1000\n",
      "1424/1424 [==============================] - 30s 21ms/step - loss: 0.0452 - accuracy: 0.9421 - val_loss: 0.0068 - val_accuracy: 0.9996\n",
      "Epoch 48/1000\n",
      "1424/1424 [==============================] - 30s 21ms/step - loss: 0.0451 - accuracy: 0.9425 - val_loss: 0.0091 - val_accuracy: 0.9999\n",
      "Epoch 49/1000\n",
      "1424/1424 [==============================] - 29s 21ms/step - loss: 0.0453 - accuracy: 0.9426 - val_loss: 0.0077 - val_accuracy: 0.9987\n",
      "Epoch 50/1000\n",
      "1424/1424 [==============================] - 30s 21ms/step - loss: 0.0451 - accuracy: 0.9427 - val_loss: 0.0091 - val_accuracy: 0.9997\n",
      "Epoch 51/1000\n",
      "1424/1424 [==============================] - 30s 21ms/step - loss: 0.0451 - accuracy: 0.9424 - val_loss: 0.0079 - val_accuracy: 0.9996\n",
      "Epoch 52/1000\n",
      "1424/1424 [==============================] - 30s 21ms/step - loss: 0.0451 - accuracy: 0.9425 - val_loss: 0.0073 - val_accuracy: 0.9999\n",
      "Epoch 53/1000\n",
      "1424/1424 [==============================] - 30s 21ms/step - loss: 0.0451 - accuracy: 0.9425 - val_loss: 0.0105 - val_accuracy: 0.9997\n",
      "Epoch 54/1000\n",
      "1424/1424 [==============================] - 30s 21ms/step - loss: 0.0449 - accuracy: 0.9422 - val_loss: 0.0074 - val_accuracy: 0.9996\n",
      "Epoch 55/1000\n",
      "1424/1424 [==============================] - 29s 21ms/step - loss: 0.0452 - accuracy: 0.9419 - val_loss: 0.0096 - val_accuracy: 0.9998\n",
      "Epoch 56/1000\n",
      "1424/1424 [==============================] - 29s 20ms/step - loss: 0.0451 - accuracy: 0.9426 - val_loss: 0.0070 - val_accuracy: 0.9999\n",
      "Epoch 57/1000\n",
      "1424/1424 [==============================] - 29s 20ms/step - loss: 0.0450 - accuracy: 0.9426 - val_loss: 0.0096 - val_accuracy: 0.9999\n",
      "Epoch 58/1000\n",
      "1424/1424 [==============================] - 30s 21ms/step - loss: 0.0451 - accuracy: 0.9424 - val_loss: 0.0057 - val_accuracy: 0.9999\n",
      "Epoch 59/1000\n",
      "1424/1424 [==============================] - 30s 21ms/step - loss: 0.0450 - accuracy: 0.9428 - val_loss: 0.0096 - val_accuracy: 0.9995\n",
      "Epoch 60/1000\n",
      "1424/1424 [==============================] - 30s 21ms/step - loss: 0.0448 - accuracy: 0.9427 - val_loss: 0.0070 - val_accuracy: 0.9998\n",
      "Epoch 61/1000\n",
      "1424/1424 [==============================] - 30s 21ms/step - loss: 0.0451 - accuracy: 0.9426 - val_loss: 0.0076 - val_accuracy: 0.9998\n",
      "Epoch 62/1000\n",
      "1424/1424 [==============================] - 30s 21ms/step - loss: 0.0449 - accuracy: 0.9428 - val_loss: 0.0095 - val_accuracy: 0.9998\n",
      "Epoch 63/1000\n",
      "1424/1424 [==============================] - 28s 20ms/step - loss: 0.0450 - accuracy: 0.9425 - val_loss: 0.0074 - val_accuracy: 0.9999\n",
      "Epoch 64/1000\n",
      "1424/1424 [==============================] - 28s 20ms/step - loss: 0.0450 - accuracy: 0.9427 - val_loss: 0.0101 - val_accuracy: 0.9998\n",
      "Epoch 65/1000\n",
      "1424/1424 [==============================] - 30s 21ms/step - loss: 0.0449 - accuracy: 0.9427 - val_loss: 0.0094 - val_accuracy: 1.0000\n",
      "Epoch 66/1000\n",
      "1424/1424 [==============================] - 29s 21ms/step - loss: 0.0449 - accuracy: 0.9427 - val_loss: 0.0075 - val_accuracy: 0.9997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as multi_head_attention_layer_call_fn, multi_head_attention_layer_call_and_return_conditional_losses, layer_normalization_layer_call_fn, layer_normalization_layer_call_and_return_conditional_losses, layer_normalization_1_layer_call_fn while saving (showing 5 of 66). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:/Users/sukegawa/Desktop/study/model/2tinvfp16_1m_3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:/Users/sukegawa/Desktop/study/model/2tinvfp16_1m_3\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 1s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "# 設定\n",
    "config = {\n",
    "    'input_dim': 33,\n",
    "    'num_heads': 4,\n",
    "    'ff_dim': 132,\n",
    "    'num_transformer_blocks': 3,\n",
    "    'dropout_rate': 0.1,\n",
    "    'dense_units': [64, 32],\n",
    "    'batch_size': 1024,\n",
    "    'epochs': 1000,\n",
    "    'patience': 20,\n",
    "    'initial_learning_rate': 1e-3,\n",
    "    'decay_steps': 10000,\n",
    "    'decay_rate': 0.9,\n",
    "    'log_dir': R'C:/Users/sukegawa/Desktop/study/logs/2tinvfp16_1m_3',\n",
    "    'model_path': R'C:/Users/sukegawa/Desktop/study/model/2tinvfp16_1m_3'\n",
    "}\n",
    "\n",
    "# 訓練データの訓練\n",
    "estimator = TransformerTrainer(config)\n",
    "train_dataset = pd.read_csv(R'C:/Users/sukegawa/Desktop/study/datasets/tinvfp/2tinvfp16_1m.csv')\n",
    "history = estimator.train(train_dataset)\n",
    "\n",
    "# テストデータの評価\n",
    "test_data = pd.read_csv(R'C:/Users/sukegawa/Desktop/study/datasets/tinvfp/2tinvfp16_1m_test.csv')\n",
    "predictions = estimator.predict(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
