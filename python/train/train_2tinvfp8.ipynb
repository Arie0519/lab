{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリのインポート\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, models\n",
    "from keras.callbacks import TensorBoard, EarlyStopping\n",
    "from keras.layers import Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformerブロック\n",
    "class TransformerBlock(layers.Layer):\n",
    "    # 初期化\n",
    "    def __init__(self, input_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=input_dim)\n",
    "        self.ffn = keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation=\"relu\"),\n",
    "            layers.Dense(input_dim),\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    # 呼び出し\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)                      # マルチヘッドアテンション層\n",
    "        attn_output = self.dropout1(attn_output, training=training) # ドロップアウト\n",
    "        out1 = self.layernorm1(inputs + attn_output)                # レイヤー正規化\n",
    "        ffn_output = self.ffn(out1)                                 # フィードフォワードネットワーク層\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)   # ドロップアウト\n",
    "        return self.layernorm2(out1 + ffn_output)                   # レイヤー正規化\n",
    "\n",
    "# Transformerモデル\n",
    "class TransformerModel(models.Model):\n",
    "    # 初期化\n",
    "    def __init__(self, config):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.config = config\n",
    "        self.normalizer = Normalization(axis=-1)\n",
    "        self.transformer_blocks = [\n",
    "            TransformerBlock(config['input_dim'], config['num_heads'], config['ff_dim'], config['dropout_rate'])\n",
    "            for _ in range(config['num_transformer_blocks'])\n",
    "        ]\n",
    "        self.global_average_pooling = layers.GlobalAveragePooling1D()\n",
    "        self.dropout = layers.Dropout(config['dropout_rate'])\n",
    "        self.dense_layers = [layers.Dense(units, activation=\"relu\") for units in config['dense_units']]\n",
    "        self.output_layer = layers.Dense(2)\n",
    "\n",
    "    # 呼び出し\n",
    "    def call(self, inputs):\n",
    "        x = self.normalizer(inputs)                         # 訓練データの正規化\n",
    "        for transformer_block in self.transformer_blocks:   # Transformerブロック\n",
    "            x = transformer_block(x)\n",
    "        x = self.global_average_pooling(x)                  # 1次元に\n",
    "        x = self.dropout(x)                                 # ドロップアウト\n",
    "        for dense_layer in self.dense_layers:               # 全結合層\n",
    "            x = dense_layer(x)\n",
    "            x = self.dropout(x)\n",
    "        return self.output_layer(x)                         # 出力層(2次元)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練\n",
    "class TransformerTrainer:\n",
    "    # 初期化\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "\n",
    "    # データの前処理\n",
    "    def preprocess_data(self, dataset):\n",
    "        data = dataset.iloc[:, list(range(4, 12)) + [14] + list(range(19, 27))]\n",
    "        label = dataset[['x_2', 'y_2']]\n",
    "        return data.values.reshape(-1, 1, 17), label.values\n",
    "\n",
    "    # モデルのビルドとコンパイル\n",
    "    def build_model(self, input_shape):\n",
    "        self.model = TransformerModel(self.config)\n",
    "        self.model.build(input_shape)\n",
    "        self.model.summary()\n",
    "        \n",
    "        # 学習率のスケジューリング\n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "            self.config['initial_learning_rate'],\n",
    "            decay_steps=self.config['decay_steps'],\n",
    "            decay_rate=self.config['decay_rate'],\n",
    "            staircase=True)\n",
    "\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        self.model.compile(optimizer=optimizer, loss=\"mean_squared_error\", metrics=[\"accuracy\"])\n",
    "\n",
    "    # 訓練\n",
    "    def train(self, train_dataset, validation_split=0.2):\n",
    "        # 訓練データの前処理\n",
    "        data, label = self.preprocess_data(train_dataset)\n",
    "        \n",
    "        # サマリーの表示\n",
    "        if self.model is None:\n",
    "            self.build_model(data.shape)\n",
    "        else:\n",
    "            self.model.summary()\n",
    "        \n",
    "        # 訓練データの正規化\n",
    "        self.model.normalizer.adapt(data)\n",
    "        \n",
    "        # コールバック関数の定義\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=self.config['patience']),\n",
    "            TensorBoard(log_dir=self.config['log_dir'])\n",
    "        ]\n",
    "\n",
    "        # データの保存\n",
    "        history = self.model.fit(\n",
    "            data, label,\n",
    "            validation_split=validation_split,\n",
    "            batch_size=self.config['batch_size'],\n",
    "            epochs=self.config['epochs'],\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        self.model.save(self.config['model_path'])\n",
    "        return history\n",
    "\n",
    "    # 予測\n",
    "    def predict(self, data):\n",
    "        preprocessed_data, _ = self.preprocess_data(data)\n",
    "        return self.model.predict(preprocessed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizatio  multiple                 35        \n",
      " n)                                                              \n",
      "                                                                 \n",
      " transformer_block (Transfor  multiple                 7310      \n",
      " merBlock)                                                       \n",
      "                                                                 \n",
      " transformer_block_1 (Transf  multiple                 7310      \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " global_average_pooling1d (G  multiple                 0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         multiple                  0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             multiple                  1152      \n",
      "                                                                 \n",
      " dense_5 (Dense)             multiple                  2080      \n",
      "                                                                 \n",
      " dense_6 (Dense)             multiple                  66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,953\n",
      "Trainable params: 17,918\n",
      "Non-trainable params: 35\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "1424/1424 [==============================] - 26s 16ms/step - loss: 0.6586 - accuracy: 0.9203 - val_loss: 0.1298 - val_accuracy: 0.9092\n",
      "Epoch 2/1000\n",
      "1424/1424 [==============================] - 23s 16ms/step - loss: 0.2244 - accuracy: 0.9338 - val_loss: 0.0627 - val_accuracy: 0.9473\n",
      "Epoch 3/1000\n",
      "1424/1424 [==============================] - 23s 16ms/step - loss: 0.1597 - accuracy: 0.9386 - val_loss: 0.0450 - val_accuracy: 0.9490\n",
      "Epoch 4/1000\n",
      "1424/1424 [==============================] - 24s 17ms/step - loss: 0.1290 - accuracy: 0.9385 - val_loss: 0.0299 - val_accuracy: 0.9640\n",
      "Epoch 5/1000\n",
      "1424/1424 [==============================] - 23s 16ms/step - loss: 0.1089 - accuracy: 0.9374 - val_loss: 0.0342 - val_accuracy: 0.9436\n",
      "Epoch 6/1000\n",
      "1424/1424 [==============================] - 23s 16ms/step - loss: 0.0959 - accuracy: 0.9361 - val_loss: 0.0231 - val_accuracy: 0.9552\n",
      "Epoch 7/1000\n",
      "1424/1424 [==============================] - 24s 17ms/step - loss: 0.0881 - accuracy: 0.9346 - val_loss: 0.0180 - val_accuracy: 0.9571\n",
      "Epoch 8/1000\n",
      "1424/1424 [==============================] - 24s 17ms/step - loss: 0.0823 - accuracy: 0.9335 - val_loss: 0.0260 - val_accuracy: 0.9605\n",
      "Epoch 9/1000\n",
      "1424/1424 [==============================] - 23s 16ms/step - loss: 0.0790 - accuracy: 0.9341 - val_loss: 0.0289 - val_accuracy: 0.9462\n",
      "Epoch 10/1000\n",
      "1424/1424 [==============================] - 23s 16ms/step - loss: 0.0763 - accuracy: 0.9332 - val_loss: 0.0213 - val_accuracy: 0.9527\n",
      "Epoch 11/1000\n",
      "1424/1424 [==============================] - 23s 16ms/step - loss: 0.0743 - accuracy: 0.9332 - val_loss: 0.0240 - val_accuracy: 0.9652\n",
      "Epoch 12/1000\n",
      "1424/1424 [==============================] - 23s 16ms/step - loss: 0.0728 - accuracy: 0.9334 - val_loss: 0.0265 - val_accuracy: 0.9787\n",
      "Epoch 13/1000\n",
      "1424/1424 [==============================] - 24s 17ms/step - loss: 0.0716 - accuracy: 0.9338 - val_loss: 0.0255 - val_accuracy: 0.9465\n",
      "Epoch 14/1000\n",
      "1424/1424 [==============================] - 23s 16ms/step - loss: 0.0707 - accuracy: 0.9337 - val_loss: 0.0224 - val_accuracy: 0.9696\n",
      "Epoch 15/1000\n",
      "1424/1424 [==============================] - 24s 17ms/step - loss: 0.0695 - accuracy: 0.9340 - val_loss: 0.0189 - val_accuracy: 0.9178\n",
      "Epoch 16/1000\n",
      "1424/1424 [==============================] - 24s 17ms/step - loss: 0.0687 - accuracy: 0.9341 - val_loss: 0.0274 - val_accuracy: 0.9371\n",
      "Epoch 17/1000\n",
      "1424/1424 [==============================] - 24s 17ms/step - loss: 0.0682 - accuracy: 0.9339 - val_loss: 0.0261 - val_accuracy: 0.9367\n",
      "Epoch 18/1000\n",
      "1424/1424 [==============================] - 23s 16ms/step - loss: 0.0681 - accuracy: 0.9339 - val_loss: 0.0200 - val_accuracy: 0.9678\n",
      "Epoch 19/1000\n",
      "1424/1424 [==============================] - 23s 16ms/step - loss: 0.0676 - accuracy: 0.9339 - val_loss: 0.0293 - val_accuracy: 0.9771\n",
      "Epoch 20/1000\n",
      "1424/1424 [==============================] - 23s 16ms/step - loss: 0.0670 - accuracy: 0.9339 - val_loss: 0.0283 - val_accuracy: 0.9851\n",
      "Epoch 21/1000\n",
      "1424/1424 [==============================] - 23s 16ms/step - loss: 0.0665 - accuracy: 0.9336 - val_loss: 0.0213 - val_accuracy: 0.9186\n",
      "Epoch 22/1000\n",
      "1424/1424 [==============================] - 23s 16ms/step - loss: 0.0658 - accuracy: 0.9335 - val_loss: 0.0244 - val_accuracy: 0.9814\n",
      "Epoch 23/1000\n",
      "1424/1424 [==============================] - 24s 17ms/step - loss: 0.0656 - accuracy: 0.9335 - val_loss: 0.0249 - val_accuracy: 0.9560\n",
      "Epoch 24/1000\n",
      "1424/1424 [==============================] - 24s 17ms/step - loss: 0.0654 - accuracy: 0.9337 - val_loss: 0.0249 - val_accuracy: 0.9339\n",
      "Epoch 25/1000\n",
      "1424/1424 [==============================] - 24s 17ms/step - loss: 0.0651 - accuracy: 0.9332 - val_loss: 0.0311 - val_accuracy: 0.9343\n",
      "Epoch 26/1000\n",
      "1424/1424 [==============================] - 24s 17ms/step - loss: 0.0652 - accuracy: 0.9335 - val_loss: 0.0240 - val_accuracy: 0.9534\n",
      "Epoch 27/1000\n",
      "1424/1424 [==============================] - 23s 16ms/step - loss: 0.0646 - accuracy: 0.9336 - val_loss: 0.0357 - val_accuracy: 0.9777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as multi_head_attention_layer_call_fn, multi_head_attention_layer_call_and_return_conditional_losses, layer_normalization_layer_call_fn, layer_normalization_layer_call_and_return_conditional_losses, layer_normalization_1_layer_call_fn while saving (showing 5 of 44). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:/Users/sukegawa/Desktop/study/model/2tinvfp8_1m_2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:/Users/sukegawa/Desktop/study/model/2tinvfp8_1m_2\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 1s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "# 設定\n",
    "config = {\n",
    "    'input_dim': 17,\n",
    "    'num_heads': 4,\n",
    "    'ff_dim': 68,\n",
    "    'num_transformer_blocks': 2,\n",
    "    'dropout_rate': 0.1,\n",
    "    'dense_units': [64, 32],\n",
    "    'batch_size': 1024,\n",
    "    'epochs': 1000,\n",
    "    'patience': 20,\n",
    "    'initial_learning_rate': 1e-3,\n",
    "    'decay_steps': 10000,\n",
    "    'decay_rate': 0.9,\n",
    "    'log_dir': R'C:/Users/sukegawa/Desktop/study/logs/2tinvfp8_1m_2',\n",
    "    'model_path': R'C:/Users/sukegawa/Desktop/study/model/2tinvfp8_1m_2'\n",
    "}\n",
    "\n",
    "# 訓練データの訓練\n",
    "estimator = TransformerTrainer(config)\n",
    "train_dataset = pd.read_csv(R'C:/Users/sukegawa/Desktop/study/datasets/tinvfp/2tinvfp8_1m.csv')\n",
    "history = estimator.train(train_dataset)\n",
    "\n",
    "# テストデータの評価\n",
    "test_data = pd.read_csv(R'C:/Users/sukegawa/Desktop/study/datasets/tinvfp/2tinvfp8_1m_test.csv')\n",
    "predictions = estimator.predict(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
